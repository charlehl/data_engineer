{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part I. ETL Pipeline for Pre-Processing the Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Python packages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Python packages \n",
    "import pandas as pd\n",
    "import cassandra\n",
    "import re\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import json\n",
    "import csv\n",
    "# Add to for timing code\n",
    "import timeit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating list of filepaths to process original event csv data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/workspace\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking your current working directory\n",
    "print(os.getcwd())\n",
    "\n",
    "# Get current folder and subfolder event data\n",
    "filepath = os.getcwd() + '/event_data'\n",
    "\n",
    "# Create a for loop to create a list of files and collect each filepath\n",
    "for root, dirs, files in os.walk(filepath):\n",
    "    # eliminate python notebook backup csv files generated\n",
    "    if(root != \"/home/workspace/event_data/.ipynb_checkpoints\"):\n",
    "    # join the file path and roots with the subdirectories using glob\n",
    "        file_path_list = glob.glob(os.path.join(root,'*'))\n",
    "    #print(file_path_list)\n",
    "# Should be 30 files\n",
    "len(file_path_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Processing the files to create the data file csv that will be used for Apache Casssandra tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Process csv using csv reader/writer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time:  0.5078429029999825\n"
     ]
    }
   ],
   "source": [
    "start = timeit.default_timer()\n",
    "# initiating an empty list of rows that will be generated from each file\n",
    "full_data_rows_list = [] \n",
    "    \n",
    "# for every filepath in the file path list \n",
    "for f in file_path_list:\n",
    "\n",
    "# reading csv file \n",
    "    with open(f, 'r', encoding = 'utf8', newline='') as csvfile: \n",
    "        # creating a csv reader object \n",
    "        csvreader = csv.reader(csvfile) \n",
    "        next(csvreader)\n",
    "        \n",
    " # extracting each data row one by one and append it        \n",
    "        for line in csvreader:\n",
    "            #print(line)\n",
    "            full_data_rows_list.append(line) \n",
    "            \n",
    "# uncomment the code below if you would like to get total number of rows \n",
    "#print(len(full_data_rows_list))\n",
    "# uncomment the code below if you would like to check to see what the list of event data rows will look like\n",
    "#print(full_data_rows_list)\n",
    "\n",
    "# creating a smaller event data csv file called event_datafile_new csv that will be used to insert data into the \\\n",
    "# Apache Cassandra tables\n",
    "csv.register_dialect('myDialect', quoting=csv.QUOTE_ALL, skipinitialspace=True)\n",
    "\n",
    "with open('event_datafile_new.csv', 'w', encoding = 'utf8', newline='') as f:\n",
    "    writer = csv.writer(f, dialect='myDialect')\n",
    "    writer.writerow(['artist','firstName','gender','itemInSession','lastName','length',\\\n",
    "                'level','location','sessionId','song','userId'])\n",
    "    for row in full_data_rows_list:\n",
    "        if (row[0] == ''):\n",
    "            continue\n",
    "        writer.writerow((row[0], row[2], row[3], row[4], row[5], row[6], row[7], row[8], row[12], row[13], row[16]))\n",
    "stop = timeit.default_timer()\n",
    "print('Time: ', (stop-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6821\n"
     ]
    }
   ],
   "source": [
    "# check the number of rows in your csv file\n",
    "with open('event_datafile_new.csv', 'r', encoding = 'utf8') as f:\n",
    "    print(sum(1 for line in f))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Process csv using pandas (same as the above cell using csv writer but using Pandas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time:  2.2619616659999906\n"
     ]
    }
   ],
   "source": [
    "start = timeit.default_timer()\n",
    "# Read in CSV with pandas instead for fun\n",
    "col_names = ['artist', 'firstName', 'gender', 'itemInSession', 'lastName', 'length', \\\n",
    "       'level', 'location', 'sessionId', 'song', 'userId']\n",
    "full_df = pd.DataFrame(columns = col_names)\n",
    "for f in file_path_list:\n",
    "    #print(f)\n",
    "    df = pd.read_csv(f)\n",
    "    df = df.dropna(subset=['artist']).reset_index(drop=True).drop(columns=['auth', 'method', 'page', 'registration', 'status', 'ts'])\n",
    "    full_df = full_df.append(df)\n",
    "\n",
    "# Convert to int32\n",
    "full_df = full_df.astype({'userId':'int32', 'sessionId': 'int32'})\n",
    "# round the length to 5 to copy the csv version\n",
    "full_df = full_df.round({'length': 5})\n",
    "# Check NaN\n",
    "print(full_df.isna().sum().sum())\n",
    "full_df.to_csv('event_datafile_new.csv', index = False)\n",
    "stop = timeit.default_timer()\n",
    "print('Time: ', (stop-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6820"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pandas doesn't count column name which is 1 row diff\n",
    "full_df.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part II. Complete the Apache Cassandra coding portion of your project. \n",
    "\n",
    "## Now you are ready to work with the CSV file titled <font color=red>event_datafile_new.csv</font>, located within the Workspace directory.  The event_datafile_new.csv contains the following columns: \n",
    "- artist \n",
    "- firstName of user\n",
    "- gender of user\n",
    "- item number in session\n",
    "- last name of user\n",
    "- length of the song\n",
    "- level (paid or free song)\n",
    "- location of the user\n",
    "- sessionId\n",
    "- song title\n",
    "- userId\n",
    "\n",
    "The image below is a screenshot of what the denormalized data should appear like in the <font color=red>**event_datafile_new.csv**</font> after the code above is run:<br>\n",
    "\n",
    "<img src=\"images/image_event_datafile_new.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating a Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This should make a connection to a Cassandra instance your local machine \n",
    "# (127.0.0.1)\n",
    "\n",
    "from cassandra.cluster import Cluster\n",
    "cluster = Cluster()\n",
    "\n",
    "# To establish connection and begin executing queries, need a session\n",
    "session = cluster.connect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Keyspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Keyspace \n",
    "try:\n",
    "    session.execute(\"\"\"\n",
    "    CREATE KEYSPACE IF NOT EXISTS sparkify_db \n",
    "    WITH REPLICATION = \n",
    "    { 'class' : 'SimpleStrategy', 'replication_factor' : 1 }\"\"\"\n",
    ")\n",
    "\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set Keyspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set KEYSPACE to the keyspace specified above\n",
    "try:\n",
    "    session.set_keyspace('sparkify_db')\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create tables to run the following queries. Since we are using Apache Cassandra, model the database tables on the queries to run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create queries to ask the following three questions of the data\n",
    "\n",
    "### 1. Give the artist, song title and song's length in the music app history that was heard during  sessionId = 338, and itemInSession  = 4\n",
    "\n",
    "\n",
    "### 2. Give only the following: name of artist, song (sorted by itemInSession) and user (first and last name) for userid = 10, sessionid = 182\n",
    "    \n",
    "\n",
    "### 3. Give every user name (first and last) in my music app history who listened to the song 'All Hands Against His Own'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Query 1:  Give the artist, song title and song's length in the music app history that was heard during \\\n",
    "## sessionId = 338, and itemInSession = 4\n",
    "\n",
    "# Description: Primary Key has two fields: sessionId is the partition key, and itemInSession is clustering key. Partitioning is done by sessionId and within that partition, rows are ordered by the itemInSession.\n",
    "        \n",
    "query = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS music_app_history \n",
    "(sessionId int, itemInSession int, artist_name varchar, song_title varchar, song_length float, PRIMARY KEY(sessionId, itemInSession))\n",
    "\"\"\"\n",
    "try:\n",
    "    session.execute(query)\n",
    "except Exception as e:\n",
    "    print(e)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Below code uses csv reader but I used pandas instead...code works but didn't like that I had to cast items for query to work properly\n",
    "##### Cells below this use pandas CSV reader method to load tables instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using CSV reader to read data and insert into DB\n",
    "file = 'event_datafile_new.csv'\n",
    "\n",
    " col_names = ['artist', 'firstName', 'gender', 'itemInSession', 'lastName', 'length', 'level', 'location', 'sessionId', 'song', 'userId']\n",
    "# Assign the INSERT statements into the `query` variable\n",
    "query = \"INSERT INTO music_app_history (sessionId, itemInSession, artist_name, song_title, song_length) \"\n",
    "query = query + \"VALUES(%s, %s, %s, %s, %s)\"\n",
    "\n",
    "with open(file, encoding = 'utf8') as f:\n",
    "    csvreader = csv.reader(f)\n",
    "    next(csvreader) # skip header\n",
    "    for line in csvreader:\n",
    "        ## Assign which column element should be assigned for each column in the INSERT statement.\n",
    "        ## For e.g., to INSERT artist_name and user first_name, you would change the code below to `line[0], line[1]`\n",
    "        session.execute(query, (int(line[8]), int(line[3]), line[0], line[9], float(line[5])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's use my pandas file instead since this preserves dtypes\n",
    "file = 'event_datafile_new.csv'\n",
    "query = \"INSERT INTO music_app_history (sessionId, itemInSession, artist_name, song_title, song_length) \"\n",
    "query = query + \"VALUES(%s, %s, %s, %s, %s)\"\n",
    "data_df = pd.read_csv(file)\n",
    "for row in data_df.itertuples(index=False):\n",
    "    #print(row[1])\n",
    "    session.execute(query, (row[8], row[3], row[0], row[9], row[5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Query to find the artist and song title and song length for session Id: 338 and for the fourth song played during the session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithless Music Matters (Mark Knight Dub) 495.30731201171875\n"
     ]
    }
   ],
   "source": [
    "## Add in the SELECT statement to verify the data was entered into the table\n",
    "query_verify = \"SELECT artist_name, song_title, song_length FROM music_app_history WHERE sessionId=338 AND itemInSession=4\"\n",
    "try:\n",
    "    rows = session.execute(query_verify)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    \n",
    "for row in rows:\n",
    "    print (row.artist_name, row.song_title, row.song_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create table and query to answer question 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Query 2: Give me only the following: name of artist, song (sorted by itemInSession) and user (first and last name)\\\n",
    "## for userid = 10, sessionid = 182\n",
    "\n",
    "# Description: Primary Key has three fields: userId is the partition key, and sessionId and itemInSession are the clustering keys. Partitioning is done by userId and within that partition, rows are ordered by the sessionId and then by itemInSession.\n",
    "query2 = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS user_app_history \n",
    "(userId int, sessionId int, itemInSession int, artist_name varchar, song_title varchar, first_name varchar, last_name varchar, PRIMARY KEY(userId, sessionId, itemInSession))\n",
    "\"\"\"\n",
    "try:\n",
    "    session.execute(query2)\n",
    "except Exception as e:\n",
    "    print(e)         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# col_names = ['artist', 'firstName', 'gender', 'itemInSession', 'lastName', 'length', 'level', 'location', 'sessionId', 'song', 'userId']\n",
    "file = 'event_datafile_new.csv'\n",
    "query = \"INSERT INTO user_app_history (userId, sessionId, itemInSession, artist_name, song_title, first_name, last_name) \"\n",
    "query = query + \"VALUES(%s, %s, %s, %s, %s, %s, %s)\"\n",
    "data_df = pd.read_csv(file)\n",
    "for row in data_df.itertuples(index=False):\n",
    "    #print(row[1])\n",
    "    session.execute(query, (row[10], row[8], row[3], row[0], row[9], row[1], row[4]))\n",
    "    \n",
    "#print(data_df.head(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Query to find the artists and song titles and user first and last name for user Id: 10 and session Id 182.\n",
    "##### Query will list all of the songs played in that session by order played using itemInSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Down To The Bone Keep On Keepin' On Sylvie Cruz\n",
      "Three Drives Greece 2000 Sylvie Cruz\n",
      "Sebastien Tellier Kilometer Sylvie Cruz\n",
      "Lonnie Gordon Catch You Baby (Steve Pitron & Max Sanna Radio Edit) Sylvie Cruz\n"
     ]
    }
   ],
   "source": [
    "query_verify = \"SELECT artist_name, song_title, first_name, last_name FROM user_app_history WHERE userId=10 AND sessionId=182\"\n",
    "try:\n",
    "    rows = session.execute(query_verify)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    \n",
    "for row in rows:\n",
    "    print (row.artist_name, row.song_title, row.first_name, row.last_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Query 3: Give every user name (first and last) in my music app history who listened to the song 'All Hands Against His Own'\n",
    "\n",
    "# Description: Primary Key has two fields: song_title is the partition key, and userId is the clustering key. Partitioning is done by song_title and within that partition, rows are ordered by the userId.\n",
    "\n",
    "query3 = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS song_app_history \n",
    "(song_title varchar, userId int, artist_name varchar, first_name varchar, last_name varchar, PRIMARY KEY(song_title, userId))\n",
    "\"\"\"\n",
    "try:\n",
    "    session.execute(query3)\n",
    "except Exception as e:\n",
    "    print(e)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# col_names = ['artist', 'firstName', 'gender', 'itemInSession', 'lastName', 'length', 'level', 'location', 'sessionId', 'song', 'userId']\n",
    "file = 'event_datafile_new.csv'\n",
    "query = \"INSERT INTO song_app_history (song_title, userId, first_name, last_name) \"\n",
    "query = query + \"VALUES(%s, %s, %s, %s)\"\n",
    "data_df = pd.read_csv(file)\n",
    "for row in data_df.itertuples(index=False):\n",
    "    #print(row[1])\n",
    "    session.execute(query, (row[9], row[10], row[1], row[4]))\n",
    "    \n",
    "#print(data_df.head(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Query to find the names of users (first and last) who listened to the song title \"All Hands Against His Own\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jacqueline Lynch\n",
      "Tegan Levine\n",
      "Sara Johnson\n"
     ]
    }
   ],
   "source": [
    "query_verify = \"SELECT first_name, last_name FROM song_app_history WHERE song_title = 'All Hands Against His Own'\"\n",
    "try:\n",
    "    rows = session.execute(query_verify)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    \n",
    "for row in rows:\n",
    "    print (row.first_name, row.last_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop the tables before closing out the sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Drop the table before closing out the sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_drop = \"DROP TABLE IF EXISTS music_app_history\"\n",
    "try:\n",
    "    rows = session.execute(query_drop)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "query_drop = \"DROP TABLE IF EXISTS user_app_history\"\n",
    "try:\n",
    "    rows = session.execute(query_drop)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    \n",
    "query_drop = \"DROP TABLE IF EXISTS song_app_history\"\n",
    "try:\n",
    "    rows = session.execute(query_drop)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Close the session and cluster connection¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.shutdown()\n",
    "cluster.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
