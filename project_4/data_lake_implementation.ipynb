{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.types import NullType\n",
    "from pyspark.sql.types import TimestampType\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.sql.types import LongType\n",
    "from pyspark.sql.types import DateType\n",
    "from pyspark.sql.functions import desc\n",
    "from pyspark.sql.functions import asc\n",
    "from pyspark.sql.functions import sum as Fsum\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "import os\n",
    "import configparser\n",
    "\n",
    "import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = configparser.ConfigParser()\n",
    "config.read_file(open('../../aws/dl.cfg'))\n",
    "\n",
    "os.environ['AWS_ACCESS_KEY_ID']= f\"{config['AWS']['AWS_ACCESS_KEY_ID']}\"\n",
    "os.environ['AWS_SECRET_ACCESS_KEY']= f\"{config['AWS']['AWS_SECRET_ACCESS_KEY']}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder\\\n",
    "                     .config(\"spark.jars.packages\",\"org.apache.hadoop:hadoop-aws:2.7.0\")\\\n",
    "                     .appName(\"sparkify_etl_pipeline\") \\\n",
    "                     .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this if using localhost to be able to access S3 bucket\n",
    "sc=spark.sparkContext\n",
    "hadoop_conf=sc._jsc.hadoopConfiguration()\n",
    "hadoop_conf.set(\"fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "hadoop_conf.set(\"fs.s3a.awsAccessKeyId\", os.getenv('AWS_ACCESS_KEY_ID'))\n",
    "hadoop_conf.set(\"fs.s3a.awsSecretAccessKey\", os.getenv('AWS_SECRET_ACCESS_KEY'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using smaller subset for experimenting\n",
    "songData = spark.read.json(\"../../data/song_data/A/*/*/*.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using smaller subset for experimenting\n",
    "logData = spark.read.json(\"../../data/log_data.tar.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "songData.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logData.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_df = logData.drop('_corrupt_record')\n",
    "log_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_df = log_df.filter(log_df['page'] == 'NextSong')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create time table\n",
    "#### start_time, hour, day, week, month, year, weekday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_df = log_df.select(\"ts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_df = time_df.withColumn(\"start_time\", F.to_timestamp(time_df[\"ts\"] / 1000))\\\n",
    "                 .withColumn(\"hour\", F.hour(\"start_time\"))\\\n",
    "                 .withColumn(\"day\", F.dayofmonth(\"start_time\"))\\\n",
    "                 .withColumn(\"week\", F.weekofyear(\"start_time\"))\\\n",
    "                 .withColumn(\"month\", F.month(\"start_time\"))\\\n",
    "                 .withColumn(\"year\", F.year(\"start_time\"))\\\n",
    "                 .withColumn(\"weekday\", F.dayofweek(\"start_time\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_df = time_df.drop(\"ts\")\n",
    "time_df = time_df.dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Won't work for notebook on local machine\n",
    "time_df.write.mode(\"overwrite\")\\\n",
    "       .partitionBy(\"year\", \"week\", \"hour\")\\\n",
    "       .parquet(\"time.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create User table\n",
    "#### user_id, first_name, last_name, gender, level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_df = log_df.select([\"userId\", \"firstName\", \"lastName\", \"gender\", \"level\", \"ts\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_df.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create window to get last entry for user\n",
    "w1 = Window.partitionBy(\"userId\").orderBy(F.asc(\"ts\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_df = user_df.withColumn(\"row\", F.row_number().over(w1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_df = user_df.filter(user_df[\"row\"] == 1).drop(\"row\", \"ts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_df = user_df.withColumnRenamed(\"userId\", \"user_id\")\\\n",
    "                 .withColumnRenamed(\"firstName\", \"first_name\")\\\n",
    "                 .withColumnRenamed(\"lastName\", \"last_name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Songs Table\n",
    "##### song_id, title, artist_id, year, duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "songData.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "song_df = songData.select([\"song_id\", \"title\", \"artist_id\", \"year\", \"duration\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "song_df = song_df.dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "song_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Artist Table\n",
    "##### artist_id, name, location, lattitude, longitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "artist_df = songData.select([\"artist_id\", \"artist_name\", \"artist_location\", \"artist_latitude\", \"artist_longitude\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "artist_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@udf\n",
    "def parseArtistLocUDF(line):\n",
    "    import re\n",
    "    PATTERN = \"^<a\\shref\"\n",
    "    match = re.search(PATTERN, line)\n",
    "    if match is None:\n",
    "        return line\n",
    "    else:\n",
    "        return NullType()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "artist_df = artist_df.dropDuplicates().withColumn(\"artist_location\", parseArtistLocUDF(\"artist_location\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "artist_df = artist_df.withColumnRenamed(\"artist_name\", \"name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "artist_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create SongPlays Table\n",
    "##### songplay_id, start_time, user_id, level, song_id, artist_id, session_id, location, user_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "songData.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "songplay_df = log_df.select(\"ts\", \"userId\", \"level\", \"sessionId\", \"location\", \"userAgent\", \"song\", \"artist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "songplay_df = songplay_df.withColumn(\"ts\", F.to_timestamp(songplay_df[\"ts\"] / 1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "songplay_df = songplay_df.withColumnRenamed(\"ts\", \"start_time\")\\\n",
    "                         .withColumnRenamed(\"userId\", \"user_id\")\\\n",
    "                         .withColumnRenamed(\"sessionId\", \"session_id\")\\\n",
    "                         .withColumnRenamed(\"userAgent\", \"user_agent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "songplay_df = songplay_df.withColumn(\"songplay_id\", F.monotonically_increasing_id())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "songplay_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to add song_id, artist_id from song and artist tables\n",
    "songplay_df.createOrReplaceTempView(\"songplay_table\")\n",
    "song_df.createOrReplaceTempView(\"song_table\")\n",
    "artist_df.createOrReplaceTempView(\"artist_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "artist_song_df = spark.sql(\"\"\"\n",
    "    SELECT a.artist_id, song_id, name, title\n",
    "    FROM artist_table AS a\n",
    "    JOIN song_table AS s ON a.artist_id = s.artist_id\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "artist_song_df.createOrReplaceTempView(\"artist_song_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "songplay_df = spark.sql(\"\"\"\n",
    "    SELECT songplay_id, start_time, user_id, level, song_id, artist_id, session_id, location, user_agent\n",
    "    FROM songplay_table AS sp\n",
    "    LEFT JOIN artist_song_table AS ast ON sp.song = ast.title AND sp.artist = ast.name\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "songplay_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "songplay_df.show(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Udacity]",
   "language": "python",
   "name": "conda-env-Udacity-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
